{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial 15 - Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Folder\n",
    "# Scheduler\n",
    "# Transfer Learning\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ants', 'bees']\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "data_dir = 'data/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=0)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st option\n",
    "\n",
    "\n",
    "Overall, this transfer learning approach leverages the features learned by a pretrained ResNet-18 model and fine-tunes them for the specific binary classification task. By adapting only the final fully connected layer and using a step-wise learning rate scheduler, it aims to efficiently train the model while preventing overfitting and ensuring convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n",
      "train Loss: 0.6024 Acc: 0.7131\n",
      "val Loss: 0.4225 Acc: 0.8758\n",
      "\n",
      "Epoch 1/1\n",
      "----------\n",
      "train Loss: 0.5311 Acc: 0.7623\n",
      "val Loss: 0.3124 Acc: 0.9281\n",
      "\n",
      "Training complete in 2m 50s\n",
      "Best val Acc: 0.928105\n"
     ]
    }
   ],
   "source": [
    "#### Finetuning the convnet ####\n",
    "# Load a pretrained model and reset final fully connected layer.\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# it resets the final fully connected layer of the model (model.fc) to adapt it to the specific binary classification task. \n",
    "# The number of input features (num_ftrs) of the fully connected layer is adjusted to match the output size required for the \n",
    "# binary classification task (2 classes in this case).\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1) # it updates learning rate\n",
    "# this means our learning rate will be multiplied by gamma after every step_size epochs. So after 7 epochs our learning rate\n",
    "# will be updated by 10% & be decreased\n",
    "\n",
    "\n",
    "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# Learning rate scheduling should be applied after optimizerâ€™s update\n",
    "# e.g., you should write your code this way:\n",
    "# for epoch in range(100):\n",
    "#     train(...)\n",
    "#     validate(...)\n",
    "#     scheduler.step()\n",
    "\n",
    "\n",
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd option\n",
    "\n",
    "* Freezing Pretrained Layers:\n",
    "\n",
    "Unlike the first approach, where the entire model was fine-tuned, in this approach, we freeze all the layers of the pretrained model except for the final fully connected layer.\n",
    "It iterates over all parameters of the model (model_conv.parameters()) and sets requires_grad = False for each parameter, except for the parameters of the final fully connected layer (model_conv.fc.parameters()).\n",
    "Freezing the pretrained layers prevents their parameters from being updated during training, effectively treating them as fixed feature extractors.\n",
    "\n",
    "\n",
    "\n",
    "* Replacing Final Fully Connected Layer:\n",
    "\n",
    "It replaces the final fully connected layer (model_conv.fc) with a new one suitable for the specific classification task. The number of input features (num_ftrs) of the new fully connected layer is adjusted accordingly.\n",
    "By replacing only the final layer, the model retains the ability to extract features from the input images while adapting the classification layer to the new task.\n",
    "\n",
    "\n",
    "* Training the Model:\n",
    "\n",
    "Finally, the model is trained (train_model function) for a specified number of epochs (num_epochs). During training, only the parameters of the final fully connected layer are updated, while the parameters of the pretrained layers remain fixed.\n",
    "This approach allows the model to leverage the features learned by the pretrained layers while adapting the final layer to the specific classification task, making it computationally efficient and potentially less prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n",
      "train Loss: 0.6795 Acc: 0.6475\n",
      "val Loss: 0.3155 Acc: 0.8627\n",
      "\n",
      "Epoch 1/1\n",
      "----------\n",
      "train Loss: 0.4967 Acc: 0.7623\n",
      "val Loss: 0.2262 Acc: 0.9085\n",
      "\n",
      "Training complete in 1m 47s\n",
      "Best val Acc: 0.908497\n"
     ]
    }
   ],
   "source": [
    "#### ConvNet as fixed feature extractor ####\n",
    "# Here, we need to freeze all the network except the final layer.\n",
    "# We need to set requires_grad == False to freeze the parameters so that the gradients are not computed in backward()\n",
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Approach 1: Fine-tuning the ConvNet \n",
    "### Updating both: Conv + fc (fully connected layer)\n",
    "\n",
    "* Description: \n",
    "In this approach, the entire pretrained model is fine-tuned by updating all of its parameters, including both the pretrained layers and the final fully connected layer.\n",
    "* Similarities:\n",
    "Both approaches use a pretrained ResNet-18 model as the basis for feature extraction.\n",
    "They both involve replacing the final fully connected layer to adapt the model to the specific classification task.\n",
    "Both approaches utilize a step-wise learning rate scheduler to gradually decrease the learning rate during training.\n",
    "* Differences:\n",
    "* * Fine-tuning: This approach fine-tunes all layers of the pretrained model, allowing the model to learn task-specific features as well as retaining previously learned features.\n",
    "* * Parameter Updates: All parameters of the model, including both pretrained layers and the final fully connected layer, are updated during training.\n",
    "* * Training Time: Fine-tuning the entire model may require more computational resources and training time compared to freezing pretrained layers.\n",
    "* Pros:\n",
    "\n",
    "* * Flexibility: Fine-tuning allows the model to adapt to the specific characteristics of the new dataset, potentially leading to improved performance.\n",
    "* * Better Feature Learning: By updating all parameters, the model can learn task-specific features that may not be present in the original pretrained model.\n",
    "* * Potentially Higher Accuracy: Fine-tuning the entire model may lead to higher accuracy compared to freezing pretrained layers, especially when the new dataset is significantly different from the original dataset used for pretraining.\n",
    "* Cons:\n",
    "\n",
    "* * Computational Cost: Fine-tuning the entire model requires more computational resources and training time compared to freezing pretrained layers.\n",
    "* * Risk of Overfitting: Fine-tuning may increase the risk of overfitting, especially if the new dataset is small or if the model is trained for too many epochs.\n",
    "\n",
    "*************************\n",
    "\n",
    "# Transfer Learning Approach 2: ConvNet as a Fixed Feature Extractor\n",
    "### Updating only fc (fully connected layer) but not Convolutional layer\n",
    "\n",
    "* Description: In this approach, only the final fully connected layer of the pretrained model is updated, while the parameters of the pretrained layers are frozen.\n",
    "* Similarities:\n",
    "Both approaches involve using a pretrained ResNet-18 model and replacing the final fully connected layer.\n",
    "They both utilize a step-wise learning rate scheduler to adjust the learning rate during training.\n",
    "\n",
    "* Differences:\n",
    "* * Freezing Pretrained Layers: In this approach, the parameters of the pretrained layers are frozen, preventing them from being updated during training.\n",
    "* * Parameter Updates: Only the parameters of the final fully connected layer are updated during training, allowing the model to adapt to the new classification task while retaining the features learned by the pretrained layers.\n",
    "* * Reduced Training Time: Freezing pretrained layers reduces the computational cost and training time compared to fine-tuning the entire model.\n",
    "* Pros:\n",
    "\n",
    "* * Computational Efficiency: Freezing pretrained layers reduces the computational cost and training time compared to fine-tuning the entire model.\n",
    "* * Reduced Risk of Overfitting: By keeping the pretrained layers fixed, there is a reduced risk of overfitting, especially when the new dataset is small.\n",
    "* * Preservation of Learned Features: Freezing pretrained layers allows the model to retain the features learned during pretraining, potentially leading to better generalization.\n",
    "* Cons:\n",
    "\n",
    "* * Less Flexibility: By keeping the pretrained layers fixed, the model may not adapt as well to the specific characteristics of the new dataset compared to fine-tuning the entire model.\n",
    "* * Limited Feature Learning: The model may be limited to the features learned by the pretrained layers and may not learn task-specific features as effectively as fine-tuning the entire model.\n",
    "\n",
    "************************\n",
    "In summary, the choice between the two transfer learning approaches depends on factors such as the size of the new dataset, computational resources available, and the desired balance between performance and computational cost. Fine-tuning the entire model offers more flexibility and potentially higher accuracy but requires more computational resources, while freezing pretrained layers is computationally efficient and reduces the risk of overfitting but may limit the adaptability of the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to apply more layers, we can do it as below\n",
    "\n",
    "# Freeze pretrained layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Add more layers at the end\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, num_classes)  # num_classes is the number of output classes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
